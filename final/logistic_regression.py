# -*- coding: utf-8 -*-
"""logistic_regression.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1MgSH6XKzoQgPSpCoQ8_djnzB5XRao1yT

These chuncks of codes are created for text analysis using logitic regression for Trump's tweet about economics from 2017-01 to 2020-10-10. 

---


Codes below do followings:
0.   Install and import packages
1.   Process tweets_examples in twitter_samples packages
2.   Training models using twitter_samples in nltk package
3.   Create and apply logistic regression models to Trump's Twitter
4.   Export csv file containing cleaned up Trump's tweets, time, and result of applying logistic regression model

References:
https://medium.com/analytics-vidhya/tweet-sentiment-analysis-using-logistic-regression-ff3dc667889f
https://medium.com/swlh/sentiment-analysis-from-scratch-with-logistic-regression-ca6f119256ab
https://www.coursera.org/learn/classification-vector-spaces-in-nlp/programming/P4CTb/assignment-logistic-regression


> Articles above provided the basic understanding of applying logistic regression on text analysis and code for process_tweet. I also used my large part of the code which I worked for Coursera NLP specialization.
"""

# install the package necessary for tweet analysis
pip install twython

# for tweet samples
import nltk
from os import getcwd

# download samples
nltk.download('twitter_samples')

# import basic package and dataset required for logistic regression
import numpy as np
import pandas as pd
from nltk.corpus import twitter_samples

# select the set of positive and negative tweets
all_positive_tweets = twitter_samples.strings('positive_tweets.json')
all_negative_tweets = twitter_samples.strings('negative_tweets.json')

# split the data into two part, one for training and one for testing (validation set) 
test_pos = all_positive_tweets[4000:]
train_pos = all_positive_tweets[:4000]
test_neg = all_negative_tweets[4000:]
train_neg = all_negative_tweets[:4000]

# concatenating positive and negative dataset for training and testing
train_x = train_pos + train_neg 
test_x = test_pos + test_neg

# combine positive and negative labels
train_y = np.append(np.ones((len(train_pos), 1)), np.zeros((len(train_neg), 1)), axis=0)
test_y = np.append(np.ones((len(test_pos), 1)), np.zeros((len(test_neg), 1)), axis=0)

# Print the shape train and test sets
print("train_y.shape = " + str(train_y.shape))
print("test_y.shape = " + str(test_y.shape))

# process_tweet takes tweet, a string with words, as input
# and returns tweets_clean, a processed tweet not including stop words but 
# including a pure string

def process_tweet(tweet):
    stemmer = PorterStemmer()
    stopwords_english = stopwords.words('english')
    # remove stock market tickers like $GE
    tweet = re.sub(r'\$\w*', '', tweet)
    # remove old style retweet text "RT"
    tweet = re.sub(r'^RT[\s]+', '', tweet)
    # remove hyperlinks
    tweet = re.sub(r'https?:\/\/.*[\r\n]*', '', tweet)
    # remove hashtags
    # only removing the hash # sign from the word
    tweet = re.sub(r'#', '', tweet)
    # tokenize tweets
    tokenizer = TweetTokenizer(preserve_case=False, strip_handles=True,
                               reduce_len=True)
    tweet_tokens = tokenizer.tokenize(tweet)

    tweets_clean = []
    sw = ["!", "#", "$", "%", 
                             "&", "'", "(", ")",
                             "*", "+", "-", ".", "/",
                             ":", ";", "<", "=", ">", "?", "@"]
    for word in tweet_tokens:
        if (word not in stopwords_english and  # remove stopwords
                word not in sw):
            # tweets_clean.append(word)
            stem_word = stemmer.stem(word)  # stemming word
            tweets_clean.append(stem_word)

    return tweets_clean

# build_freqs takes tweets, a list of tweets, and ys, a entiment label (binary value,
# 1 for positive sentiment and 0 for negative sentiment) and returns a dictionary mapping
# each pair of word and sentiment to its frequency

def build_freqs(tweets, ys):
    
    # Convert np array to list since zip needs an iterable.
    # The squeeze is necessary or the list ends up with one element.
    # Also note that this is just a NOP if ys is already a list.
    yslist = np.squeeze(ys).tolist()

    # Start with an empty dictionary and populate it by looping over all tweets
    # and over all processed words in each tweet.
    freqs = {}
    for y, tweet in zip(yslist, tweets):
        for word in process_tweet(tweet):
            pair = (word, y)
            if pair in freqs:
                freqs[pair] += 1
            else:
                freqs[pair] = 1

    return freqs

# create frequency dictionary for training data
freqs = build_freqs(train_x, train_y)

# sigmoid function takes n, a numerical value, as input
# and returns value between 0 and 1
def sigmoid(n):
  return 1/(1+np.exp(-n))

# gradient descent for training data and returns cost function and updated theta
def gds(x, y, theta, alpha, iters):
  nrow = len(x) # length of x
  for i in range(0, nrow):
    z = np.dot(x, theta) # theta * x 
    h = sigmoid(z)       # multiply activation function to theta * x
    cf = (-1/nrow)*(np.dot(y.transpose(), np.log(h)) 
    + np.dot((1-y).transpose(), np.log(1-h)))  # calculate binary cross entropy 
    theta = theta - (alpha * np.dot(x.transpose(), (h - y)) / nrow) # updating theta

  cf = float(cf)
  return cf, theta

# extract_features takes a tweet and freqs {(word, label)} as input and returns
# a feature vector of dimension(1, 3)

def extract_features(tweet, freqs):
    
    # process_tweet tokenizes, stems, and removes stopwords
    word_l = process_tweet(tweet)
    
    # 3 elements in the form of a 1 x 3 vector
    x = np.zeros((1, 3)) 
    
    #bias term is set to 1
    x[0,0] = 1 
    
    # loop through each word in the list of words
    for word in word_l:
        
        # increment the word count for the positive label 1
        x[0,1] += freqs.get((word, 1.0), 0)
        
        # increment the word count for the negative label 0
        x[0,2] += freqs.get((word, 0.0), 0)

    assert(x.shape == (1, 3))
    return x

# collect the features 'x' and stack them into a matrix 'X'
X = np.zeros((len(train_x), 3))
for i in range(len(train_x)):
    X[i, :]= extract_features(train_x[i], freqs)

# training labels corresponding to X
Y = train_y

# Apply gradient descent
J, theta = gds(X, Y, np.zeros((3, 1)), 1e-9, 1500)

# predict_tweet takes a tweet, freqs {(word, label)}, and theta as input and 
# returns a prediction value which is used to determine whether given tweet is 
# positive or negative
def predict_tweet(tweet, freqs, theta):
    
    # extract the features of the tweet and store it into x
    x = extract_features(tweet, freqs)
    
    # make the prediction using x and theta
    y_pred = sigmoid(np.dot(x, theta))
    
    return y_pred

# logistic_regression takes test data (test_x, test_y), freqs, and theta as input
# and returns accuracy score (# of classification correct) / (total number of data)
def test_logistic_regression(test_x, test_y, freqs, theta):

    # the list for storing predictions
    y_hat = []
    
    for tweet in test_x:
        # get the label prediction for the tweet
        y_pred = predict_tweet(tweet, freqs, theta)
        
        if y_pred > 0.5:  # if y_pred > 0.5
            y_hat.append(1.0) # append 1.0 to y_hat
        else:             # else
            y_hat.append(0.0) # append 0.0 to y_hat

    # With the above implementation, y_hat is a list, but test_y is (m,1) array
    # convert both to one-dimensional arrays in order to compare them using the '==' operator
    accuracy = (y_hat==np.squeeze(test_y)).sum()/len(test_x)
    
    return accuracy

tmp_accuracy = test_logistic_regression(test_x, test_y, freqs, theta)
print(f"Logistic regression model's accuracy = {tmp_accuracy:.4f}")

# apply models and create modified version of data
Trump = pd.read_csv("/content/trump_tweet_original.csv")

Trump_tweet = Trump['text']

# storing value of the result of the text analysis using a created model to y_result and
# numerical value to y_prediction
y_result = []
y_prediction = []
for tweet in Trump_tweet:
        # get the label prediction for the tweet
        y_pred = predict_tweet(tweet, freqs, theta)
        
        if y_pred > 0.5:
            # append 1.0 to the list
            y_result.append(1.0)
            y_prediction.append(y_pred)
        else:
            # append 0 to the list
            y_result.append(0.0)
            y_prediction.append(y_pred)

# overall positive scores on Trump's tweets about economics
sum(y_result) / len(y_result)

# data wrangling for time
Trump_time = Trump['created_at']
Trump_time = Trump_time.str.split("-")

temp = []     # temp for holding (year date)
year = []     # hold years tweet made
hour = []     # hold hours tweet made
month = []    # hold months tweet made
date = []     # hold dates tweet made

for time in Trump_time:
  temp.append(time[2])    # store time in temp
  month.append(time[0])   # store month in month
  date.append(time[1])    # store date in date

for time in temp:   
  lst = time.split()
  year.append(lst[0])     # store year in year
  lst2 = lst[1].split(":")
  hour.append(lst2[0])    # store hour in hour

# collect the data for export
d = {'year': year, 'month': month, 'date': date, 'hour': hour,
     'tweet': Trump_tweet, 'pos or neg': y_result, 
     'logistic regression': y_prediction}

df = pd.DataFrame(data=d)

# export newly created dataframe as trump_tweet_mod.csv
df.to_csv("trump_tweet_mod.csv", index = False)